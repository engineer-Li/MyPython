{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年1月05日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 作业截止时间\n",
    "此次作业截止时间为 2020.01.19日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：每道题是否回答完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 识别发票为可填报的单据、自动驾驶、人脸识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 1. Github：在github上创建一个仓库，在本机用命令行或Git UI把仓库拉取到本地，每次或者有大的代码修改时，注意及时推送到远程服务器<br/>\n",
    "2. Jupyter and Pycharm是python的开发环境，可以解释并运行python代码。当然也有其他IDE，譬如vscode, vscode内置Jupyter note book插件，可以直接识别ipynb文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:两个或者多个事件发生的概率关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:天气预测、扑克比大小游戏、输入文字合理性验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:Why do we use probability: 把现实发生的事物进行数据化的解析，以达到预测的目的<br/>\n",
    "difficult points：基于语法的判断比较复杂，因为人说话的方式不一定是按照语法的规则的，但是其他人也能听懂，每种语言的语法都不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 一段话中所有的文字连在一起出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 判断机器生成的语言是否准确（概率更高）、输入文字语法排错、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 在一句话中把句子拆分成词组，这些词组组合成一句话的概率是1-gram<br/>\n",
    "1. 把一句话进行切词<br/>\n",
    "2. 并分别计算这些词组出现的概率<br/>\n",
    "3. 计算这些词组组成这句话的概率<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 优点：计算简单，时间空间复杂度低<br/>\n",
    "缺点：只计算了每个词组的概率，但是如果两个词语组合在一起可能会出现逻辑不通"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:在一句话中把相邻词语两两组合，这些两两组合的词语形成这句话的概率就是2-gram<br/>\n",
    "1. 把一句话进行切词<br/>\n",
    "2. 把每两个相邻词组合在一起，并分别计算这两个词相邻的概率<br/>\n",
    "3. 把这些两两的组合练成一句话的，并计算这句话在这些两两组合词语上的概率<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分\n",
    "\n",
    "github: https://github.com/haojiefu007/MyPython/blob/master/Nlp/assignment_1/Assignment-01.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定语从句语法\n",
    "mygrammar_1 = '''\n",
    "mysentences => sentences attribute sentences\n",
    "sentences => sentence sentences | sentence\n",
    "sentence => subject predicate adj* object adv ,\\n\n",
    "attribute => as | that | which\n",
    "subject => I | He | She | Lucy | Lili\n",
    "predicate => like | buy | read | find | swim\n",
    "object => breakfast | book\n",
    "adj* => adj* | adj\n",
    "adj => long | delicious | red | yellow | bule | wide\n",
    "adv => happily | sadly | quietly | fast | in the river\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygrammar_2 = '''\n",
    "战斗 => 施法  ， 结果 。\n",
    "施法 => 主语 动作 技能 \n",
    "结果 => 主语 获得 效果\n",
    "主语 => 龙太子 | 虎头怪 | 英女侠 | 骨精灵 | 梦灵珑 | 律法女娲 | 超级神羊 | 灵符女娲 | 黑熊精 | 大海龟 | 巨蛙\n",
    "动作 => 施放 | 使用 | 召唤 \n",
    "技能 => 横扫千军 | 鹰击 | 龙卷雨击 | 善恶有报 | 上古灵符 | 破血狂攻 | 罗汉金钟 | 晶清决 | 普通攻击 | 金香玉 | 定神香 | 珍露酒\n",
    "获得 => 损失 | 获得 \n",
    "效果 => 数值 状态\n",
    "数值 => 1 | 1000 |5000 | 100 \n",
    "状态 => 气血 | 魔法 | 愤怒\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得语法字典函数\n",
    "def getGrammarDict(gram :str, linesplit = \"\\n\", gramsplit = \"=>\"):\n",
    "    #定义字典\n",
    "    grammar = {}\n",
    "\n",
    "    for line in gram.split(linesplit):\n",
    "        if not line.strip(): continue\n",
    "        expr, statement = line.split(gramsplit)\n",
    "        grammar[expr.strip()] = [i.split() for i in statement.split(\"|\")]\n",
    "    return grammar\n",
    "\n",
    "import random\n",
    "chioice = random.choice\n",
    "\n",
    "#生成句子函数\n",
    "def generate(gramdict : dict, target : str, isEng = False):\n",
    "    if target not in gramdict : return target\n",
    "    find = chioice(gramdict[target])\n",
    "    blank = ''\n",
    "    if isEng : blank = ' '\n",
    "    return blank.join(generate(gramdict, t, isEng) for t in find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'He read delicious book happily , as Lili swim yellow breakfast happily , Lucy like yellow breakfast quietly ,'"
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test1\n",
    "generate(getGrammarDict(mygrammar_1),\"mysentences\" , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'骨精灵召唤上古灵符，巨蛙损失1000愤怒。'"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test2\n",
    "generate(getGrammarDict(mygrammar_2),\"战斗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成多句\n",
    "def generate_n(gram : str,target :str ,times = 10, isEng = False):\n",
    "    ret = []\n",
    "    for i in list(range(times)):\n",
    "        sentance = generate(getGrammarDict(gram),target,isEng)\n",
    "        strlist = list(sentance)\n",
    "        if strlist[-1] == ',' :\n",
    "            strlist[-1] = '.'\n",
    "        ret.append(sentance)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "She find long book sadly , as Lucy swim red book happily , Lili read delicious breakfast fast ,\n\n\nI read long breakfast quietly , which He like bule breakfast happily ,\n\n\nHe read long breakfast sadly , as He swim delicious breakfast happily ,\n\n\nLili find delicious book sadly , Lili swim yellow book sadly , which He read red book fast , Lili read delicious book happily , Lili read yellow breakfast sadly , I find bule book fast ,\n\n\nI buy red breakfast in the river , as Lucy find yellow book happily ,\n\n\n"
    }
   ],
   "source": [
    "#Test generate_n english\n",
    "sentencelist = generate_n(mygrammar_1,\"mysentences\",5,isEng = True)\n",
    "for i in range(len(sentencelist)):\n",
    "    print(sentencelist[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "巨蛙施放鹰击，骨精灵损失1愤怒。\n\n\n梦灵珑使用普通攻击，律法女娲损失5000气血。\n\n\n龙太子使用善恶有报，灵符女娲损失5000魔法。\n\n\n龙太子召唤破血狂攻，骨精灵损失1000气血。\n\n\n"
    }
   ],
   "source": [
    "# Test generate_n chinese\n",
    "sentencelist = generate_n(mygrammar_2,\"战斗\",4,isEng = False)\n",
    "\n",
    "for i in range(len(sentencelist)):\n",
    "    print(''.join(sentencelist[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '第一步' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-81dbc2be7391>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m第一步\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '第一步' is not defined"
     ]
    }
   ],
   "source": [
    "第一步readscv2file方法读取原始数据，清理里后另存到movie_comments.txt\n",
    "\n",
    "第二步进行切词，1gram另存到movie_comments_cut1gram.txt\n",
    "\n",
    "2gram另存到movie_comments_cut2gram.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入组件\n",
    "import pandas as pd #处理文本\n",
    "import jieba #切词\n",
    "import re #正则\n",
    "\n",
    "#正则匹配文本\n",
    "def textfix(string):\n",
    "    return re.findall(\"\\w+\",string)\n",
    "\n",
    "#读取原始csv文件，并另存到xxx.txt文件\n",
    "def readscv2file(filename : str, contentcolumn : str):\n",
    "    filenamelist = list(filename.split('.'))\n",
    "    if str(filenamelist[-1]) != \"csv\":\n",
    "        raise RuntimeError('只能读取csv文件')\n",
    "\n",
    "    content = pd.read_csv(filename1)#read csv file\n",
    "    content.head()\n",
    "    articles = content[contentcolumn].tolist() #get column\n",
    "\n",
    "    articles_clean = [''.join(textfix(str(a)))for a in articles]#去除多余字符\n",
    "\n",
    "    #替换另存文件的格式为txt\n",
    "    filenamelist[-1] = 'txt'\n",
    "    newfilename = '.'.join(filenamelist)\n",
    "\n",
    "    #保存文件，使用encoding='utf-8 sig'\n",
    "    with open(newfilename, 'w', encoding='utf-8 sig') as f:\n",
    "        for a in articles_clean:\n",
    "            f.write(a + '\\n')\n",
    "\n",
    "    return newfilename\n",
    "\n",
    "\n",
    "#切词方法\n",
    "def cut(string): \n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "#进行1gram、2gram切词处理，另存到xxx_cut1gram.txt，xxx_cut2gram.txt\n",
    "def cut2file(filename : str):\n",
    "    filenamelist = list(filename.split('.'))\n",
    "    if str(filenamelist[-1]) != \"txt\":\n",
    "        raise RuntimeError('只能切txt文件')\n",
    "    token_1gram = []\n",
    "    token_2gram = []\n",
    "    #读取第一步的文件进行分词处理，保存到token_1gram\n",
    "    for i,line in enumerate(open(filename,encoding='utf-8 sig')):\n",
    "        if i % 50000 == 0 : print(\"read {0} lines\".format(i))\n",
    "        token_1gram += cut(line)\n",
    "\n",
    "    print(\"1gram process finished!\")\n",
    "    #从1gram循环拼接2gram\n",
    "    token_2gram = [''.join(textfix(''.join(token_1gram[i:i+2]))) for i in range(len(token_1gram[:-2]))]\n",
    "    print(\"2gram process finished!\")\n",
    "\n",
    "    #保存文件，使用encoding='utf-8 sig'\n",
    "    #获得另存1gram文件名，xxx_cut1gram.txt\n",
    "    filenamelist[-2] = str(filenamelist[-2]) + '_cut1gram'\n",
    "    cut1gramfilename = '.'.join(filenamelist)\n",
    "    #另存1gram\n",
    "    with open(cut1gramfilename, 'w', encoding='utf-8 sig') as f:\n",
    "        for a in token_1gram:\n",
    "            f.write(a + '\\n')\n",
    "    print(\"1gram save finished!\")\n",
    "\n",
    "    #另存1gram\n",
    "    #获得另存2gram文件名，xxx_cut2gram.txt\n",
    "    filenamelist[-2] = filenamelist[-2].replace('_cut1gram', '_cut2gram')\n",
    "    cut2gramfilename = '.'.join(filenamelist)\n",
    "    with open(cut2gramfilename, 'w', encoding='utf-8 sig') as f:\n",
    "        for a in token_2gram:\n",
    "            f.write(a + '\\n')\n",
    "    print(\"2gram save finished!\")\n",
    "\n",
    "    return token_1gram,token_2gram,cut1gramfilename,cut2gramfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\jeffb\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\core\\interactiveshell.py:3242: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n  if (await self.run_code(code, result,  async_=asy)):\nread 0 lines\nread 50000 lines\nread 100000 lines\nread 150000 lines\nread 200000 lines\nread 250000 lines\n1gram process finished!\n2gram process finished!\n1gram save finished!\n2gram save finished!\n"
    }
   ],
   "source": [
    "#执行第一步、第二部\n",
    "\n",
    "filename1 = 'movie_comments.csv'\n",
    "filename2 = 'train.txt'\n",
    "\n",
    "newname = readscv2file(filename1,'comment')\n",
    "\n",
    "#返回1gram数组、2gram数组、1gram切词文件名、2gram切词文件名\n",
    "cut1gram,cut2gram,cut1gramfile,cut2gramfile = cut2file(newname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "第三步，读取1gram、2gram文件，进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter #统计插件\n",
    "\n",
    "#1gram概率\n",
    "def prob_1gram(word): \n",
    "    if word in words_count_1gram:\n",
    "        return words_count_1gram[word] / len(words_count_1gram)\n",
    "\n",
    "    #此处对课程代码修改，由于数据量较小，需要判断存不存在\n",
    "    else:\n",
    "        return 1 / len(words_count_1gram)\n",
    "\n",
    "#2gram概率\n",
    "def prob_2gram(word1, word2):\n",
    "    if word1 + word2 in words_count_2gram: \n",
    "        return words_count_2gram[word1+word2] / words_count_1gram[word2]\n",
    "    else:\n",
    "        return 1 / len(words_count_2gram)\n",
    "\n",
    "#整个语句的概率\n",
    "def sentenceprob(sentence :str):\n",
    "    wordlist = cut(sentence)\n",
    "    prob = 1.\n",
    "\n",
    "    for i,word in enumerate(wordlist[:-1]):\n",
    "        prob *= prob_2gram(word,wordlist[i+1])\n",
    "\n",
    "    prob *= prob_1gram(wordlist[-1])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过上面返回的切词数据，获得切词统计\n",
    "words_count_1gram = Counter(cut1gram)\n",
    "words_count_2gram = Counter(cut2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "生成句子：大海龟施放破血狂攻，龙太子损失5000气血。\n句子概率：3.825773425362857e-68\n\n生成句子：龙太子施放上古灵符\n句子概率：1.3295278809793086e-24\n\n生成句子：Lili like long book sadly , Lili buy delicious book quietly , She find red breakfast fast , Lili swim bule breakfast happily , She read bule book quietly , as He find long book quietly ,\n句子概率：0.0\n"
    }
   ],
   "source": [
    "#进行生成句子测试，由于使用游戏语句，建议多生成几次\n",
    "newsententce = generate(getGrammarDict(mygrammar_2),\"战斗\")\n",
    "print(\"生成句子：{0}\".format(newsententce))\n",
    "print(\"句子概率：{0}\\n\".format(sentenceprob(newsententce)))\n",
    "\n",
    "newsententce = generate(getGrammarDict(mygrammar_2),\"施法\")\n",
    "print(\"生成句子：{0}\".format(newsententce))\n",
    "print(\"句子概率：{0}\\n\".format(sentenceprob(newsententce)))\n",
    "\n",
    "newsententce = generate(getGrammarDict(mygrammar_1),\"mysentences\" , True)\n",
    "print(\"生成句子：{0}\".format(newsententce))\n",
    "print(\"句子概率：{0}\".format(sentenceprob(newsententce)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 5]"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(1, 4), (2, 5), (4, 4), (5, 0)]"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(5, 0), (1, 4), (4, 4), (2, 5)]"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(2, 5), (1, 4), (4, 4), (5, 0)]"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(): # you code here\n",
    "    ret = []\n",
    "    sentencelist = generate_n(mygrammar_2,\"战斗\",4,isEng = False)\n",
    "    for i in range(len(sentencelist)):\n",
    "        ret.append((sentencelist[i],sentenceprob(sentencelist[i])))\n",
    "        \n",
    "    ret = sorted(ret,key=lambda u : u[1],reverse=True)\n",
    "    return ret[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "巨蛙施放善恶有报，骨精灵损失5000愤怒。\n"
    }
   ],
   "source": [
    "#测试\n",
    "print(generate_best())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 模型问题：数据量比较少，2gram有一定的局限性，两个词语连到一起是关联的，但是三个词语连接到一起可能就没有逻辑性了\n",
    "\n",
    "提升：尝试编写3gram的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer: https://github.com/haojiefu007/MyPython/blob/master/Nlp/assignment_1/assignment-01-optional-pattern-match.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer: 由于工作的原因，还没有看论文，后面看了论文会发到邮箱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit40dc0dc0f4f8425f82bc2b10d91bf305"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}